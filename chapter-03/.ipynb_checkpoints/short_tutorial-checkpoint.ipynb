{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor(2, 2) # 2x2 matrix\n",
    "x = torch.Tensor([[1, 2], [3, 4]])\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = [[1, 2], [3, 4]]\n",
    "x = np.array(x)\n",
    "x = torch.from_numpy(x)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x=\\begin{bmatrix}\n",
    "1, 2 \\\\\n",
    "3, 4\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.FloatTensor(2, 2)\n",
    "y = torch.FloatTensor(2, 2)\n",
    "y.requires_grad_(True) # if gradients need to be computed for this Tensor; autograd에 모든 연산들을 추적해야 된다고 얘기\n",
    "\n",
    "z = (x + y) + torch.FloatTensor(2, 2) # x+y에 해당하는 텐서가 생성되어 연산 그래프에 할당되고 새롭게 생성된 텐서를 더해준 뒤 z에 할당\n",
    "                                      # z로부터 역전파 수행  -> 이미 생성된 연산 그래프를 따라서 미분 값을 전달 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.FloatTensor(2, 2)\n",
    "y = torch.FloatTensor(2, 2)\n",
    "y.requires_grad_(True)\n",
    "\n",
    "with torch.no_grad(): # 기울기(gradient)를 구할 필요가 없을 때\n",
    "                      # 역전파 알고리즘 수행이 필요없는 비 학습 과정(prediction, inference(추론))등을 수행할 때 유용\n",
    "    z = (x + y) + torch.FloatTensor(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{gathered}\n",
    "y = xW+ b \\\\\n",
    "\\text{where }x\\in\\mathbb{R}^{M\\times N},W\\in\\mathbb{R}^{N\\times P}\\text{ and }b\\in\\mathbb{R}^P. \\\\\n",
    "\\text{Thus, }y\\in\\mathbb{R}^{M\\times P}.\n",
    "\\end{gathered}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "y&=f(x; \\theta)\\text{ where }\\theta=\\{W, b\\}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 이 경우엔 역전파 알고리즘을 통한 학습 불가\n",
    "\n",
    "def linear(x, W, b):\n",
    "    y = torch.mm(x, W) + b\n",
    "\n",
    "    return y\n",
    "\n",
    "x = torch.FloatTensor(16, 10)\n",
    "W = torch.FloatTensor(10, 5)\n",
    "b = torch.FloatTensor(5)\n",
    "\n",
    "y = linear(x, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 앞서 구현한 linear 함수 대신에 myLinear라는 클래스를 nn.Module을 상속받아 선언하고, 이를 사용하여 똑같은 기능 구현\n",
    "\n",
    "class MyLinear(nn.Module): # 상속\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = torch.FloatTensor(input_size, output_size)\n",
    "        self.b = torch.FloatTensor(output_size)\n",
    "\n",
    "    def forward(self, x): # Feed-forward랑 같은 함수\n",
    "        y = torch.mm(x, self.W) + self.b\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(16, 10) # 10개의 element를 가진 벡터를 16개 가진 행렬\n",
    "linear = MyLinear(10, 5) # 10 = input_size, 5 = output_size\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "params = [p.size() for p in linear.parameters()] # parameters() is an iterator that returns the parameters that need training in the called module\n",
    "print(params) # linear 모듈 내의 학습이 필요한 파라미터들의 크기를 size() 함수를 통해 확인\n",
    "# 빈 리스트가 뜸 = linear 모듈 내에 학습 가능한 파라미터가 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고: http://pytorch.org/docs/master/nn.html?highlight=parameter#parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyLinear, self).__init__()\n",
    "        \n",
    "        # 신경망의 학습 파라미터는 단순한 텐서가 아님 -> 파라미터로 등록되어야 함 -> 텐서를 Parameter 클래스로 감싸야 함\n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_size, output_size), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_size), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.mm(x, self.W) + self.b\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([10, 5]), torch.Size([5])]\n"
     ]
    }
   ],
   "source": [
    "linear = MyLinear(10, 5)\n",
    "params = [p.size() for p in linear.parameters()]\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyLinear, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_size, output_size) # 간단하게 nn.Linear 클래스를 사용해 W와 b 대체\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLinear(\n",
      "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "linear = MyLinear(10, 5)\n",
    "print(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward (Back-propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = 100 # 원하는 값\n",
    "\n",
    "x = torch.FloatTensor(16, 10)\n",
    "linear = MyLinear(10, 5)\n",
    "y = linear(x) # forward한 결과값\n",
    "loss = (objective - y.sum())**2 # 오류\n",
    "\n",
    "# 여기서 기울기는 오차임 왜냐면 gradient descent 그래프에서 기울기가 오차라서\n",
    "loss.backward() # 기울기(=loss) 구하기, loss는 scalar여야함 (벡터, 행렬 x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train() and eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLinear(\n",
       "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training...\n",
    "linear.eval()\n",
    "# Do some inference process.\n",
    "linear.train()\n",
    "# Restart training, again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}_{\\text{MSE}}(\\hat{y}, y)=\\frac{1}{N}\\sum^N_{i=1}{(\\hat{y}_i - y_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# MyModel이라는 모듈 선언\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "# 임의의 함수 f가 동작한다고 가정 -> 이때 함수 f가 내부적으로 어떻게 동작하는지 알고자 함 -> 그럼 loss function을 최소로 만드는 파라미터 theta를 찾아서 함수 f를 근사해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{gathered}\n",
    "y=f(x_1, x_2, x_3) = 3x_1 + x_2 - 2x_3 \\\\\n",
    "\\hat{y}=\\tilde{f}(x_1,x_2,x_3;\\theta) \\\\\n",
    "\\hat{\\theta}=\\underset{\\theta\\in\\Theta}{\\text{argmin }}\\mathcal{L}(\\hat{y},y)\n",
    "\\end{gathered}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 함수를 파이썬으로 구현하기\n",
    "def ground_truth(x): # y\n",
    "    return 3 * x[:, 0] + x[:, 1] - 2 * x[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 텐서를 입력받아 feed-forwarding한 후, back-propagation algorithm을 수행하여 G.D.의 한 스텝을 수행하는 함수\n",
    "def train(model, x, y, optim):\n",
    "    # initialize gradients in all parameters in module.\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # feed-forward\n",
    "    y_hat = model(x)\n",
    "    # get error between answer and inferenced.\n",
    "    loss = ((y - y_hat)**2).sum() / x.size(0)\n",
    "\n",
    "    # back-propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # one-step of gradient descent\n",
    "    optim.step()\n",
    "\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter 설정\n",
    "batch_size = 1\n",
    "n_epochs = 1000\n",
    "n_iter = 10000\n",
    "\n",
    "model = MyModel(3, 1)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.1)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.0449e-06) tensor(0.9000) tensor(0.8689)\n"
     ]
    }
   ],
   "source": [
    "# 위의 값을 사용하여 평균 손실값이 0.001보다 작아질 때까지 훈련\n",
    "for epoch in range(n_epochs):\n",
    "    avg_loss = 0\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        x = torch.rand(batch_size, 3) # Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)\n",
    "                                      # torch.rand(2,3) = 2 x 1 tensor with 3 elements in each matrix\n",
    "        y = ground_truth(x.data) # y will be a Tensor that shares the same data with x, \n",
    "                                 # is unrelated with the computation history of x, and has requires_grad=False.\n",
    "\n",
    "        loss = train(model, x, y, optim)\n",
    "\n",
    "        avg_loss += loss\n",
    "        avg_loss = avg_loss / n_iter\n",
    "\n",
    "    # simple test sample to check the network.\n",
    "    x_valid = torch.FloatTensor([[.3, .2, .1]])\n",
    "    y_valid = ground_truth(x_valid.data)\n",
    "\n",
    "    model.eval()\n",
    "    y_hat = model(x_valid)\n",
    "    model.train()\n",
    "\n",
    "    print(avg_loss, y_valid.data[0], y_hat.data[0, 0])\n",
    "\n",
    "    if avg_loss < .001: # finish the training if the loss is smaller than .001.\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type torch.cuda.FloatTensor not available. Torch not compiled with CUDA enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l4/ngfzh79n7kl3fb2wpwpc9zj80000gn/T/ipykernel_31435/4159002020.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Note that tensor is declared in torch.cuda.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# .cuda() let module move to GPU memory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: type torch.cuda.FloatTensor not available. Torch not compiled with CUDA enabled."
     ]
    }
   ],
   "source": [
    "# Note that tensor is declared in torch.cuda.\n",
    "x = torch.cuda.FloatTensor(16, 10)\n",
    "linear = MyLinear(10, 5)\n",
    "# .cuda() let module move to GPU memory.\n",
    "linear.cuda()\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
